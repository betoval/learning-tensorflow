{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "titanic-estimator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPhn2qag+9A7iscLsVg4kQA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/betoval/learning-tensorflow/blob/master/titanic-estimator-ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xfVsuFP-V3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoAC6VKHcA8n",
        "colab_type": "code",
        "outputId": "e896e577-e92b-4ab3-f41f-9775dc41250b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "#we use pandas to load the csv files\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "#show some of the data using pandas\n",
        "train.head(10)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Moran, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330877</td>\n",
              "      <td>8.4583</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>McCarthy, Mr. Timothy J</td>\n",
              "      <td>male</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17463</td>\n",
              "      <td>51.8625</td>\n",
              "      <td>E46</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Palsson, Master. Gosta Leonard</td>\n",
              "      <td>male</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>349909</td>\n",
              "      <td>21.0750</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
              "      <td>female</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>347742</td>\n",
              "      <td>11.1333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
              "      <td>female</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>237736</td>\n",
              "      <td>30.0708</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500   NaN         S\n",
              "1            2         1       1  ...  71.2833   C85         C\n",
              "2            3         1       3  ...   7.9250   NaN         S\n",
              "3            4         1       1  ...  53.1000  C123         S\n",
              "4            5         0       3  ...   8.0500   NaN         S\n",
              "5            6         0       3  ...   8.4583   NaN         Q\n",
              "6            7         0       1  ...  51.8625   E46         S\n",
              "7            8         0       3  ...  21.0750   NaN         S\n",
              "8            9         1       3  ...  11.1333   NaN         S\n",
              "9           10         1       2  ...  30.0708   NaN         C\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpyzK_YOs11u",
        "colab_type": "text"
      },
      "source": [
        "We note that we have the word \"NaN\" in the 'Age' feature, which means that we are missing those values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIRDhM20ucos",
        "colab_type": "code",
        "outputId": "a73fc266-43d6-444e-cfcf-7a8d5661121a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "total = train.isnull().sum().sort_values(ascending=False)\n",
        "total.head()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Cabin       687\n",
              "Age         177\n",
              "Embarked      2\n",
              "Fare          0\n",
              "Ticket        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Afnaghv44M",
        "colab_type": "code",
        "outputId": "871767ba-0527-4e92-a881-2bae7da4fafa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train['Embarked'].value_counts()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "S    644\n",
              "C    168\n",
              "Q     77\n",
              "Name: Embarked, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O91cbkHywBKA",
        "colab_type": "text"
      },
      "source": [
        "Here, we note that we are missing 177 values of Age, 687 of Cabin, and 2 of Embarked. As a first attemp we will replace the missing values of Age with the value of the \"mean age\" and the Embarked values with S, which is the most common."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChgstGPamKAV",
        "colab_type": "code",
        "outputId": "d2b660e2-87e2-4b9d-c4c1-985080c5cb30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "train.info()"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  891 non-null    int64  \n",
            " 1   Survived     891 non-null    int64  \n",
            " 2   Pclass       891 non-null    int64  \n",
            " 3   Name         891 non-null    object \n",
            " 4   Sex          891 non-null    object \n",
            " 5   Age          714 non-null    float64\n",
            " 6   SibSp        891 non-null    int64  \n",
            " 7   Parch        891 non-null    int64  \n",
            " 8   Ticket       891 non-null    object \n",
            " 9   Fare         891 non-null    float64\n",
            " 10  Cabin        204 non-null    object \n",
            " 11  Embarked     889 non-null    object \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtjFmkipn3qL",
        "colab_type": "text"
      },
      "source": [
        "We can see that the data has 11 features + Survived (the feature we are interested in).\n",
        "\n",
        "Below, we can examine the statistics of the data using `pd.DataFrame.describe`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M4xme6Qr-vn",
        "colab_type": "code",
        "outputId": "0050b682-d718-4988-9d70-05e0b12c9db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>714.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>446.000000</td>\n",
              "      <td>0.383838</td>\n",
              "      <td>2.308642</td>\n",
              "      <td>29.699118</td>\n",
              "      <td>0.523008</td>\n",
              "      <td>0.381594</td>\n",
              "      <td>32.204208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>257.353842</td>\n",
              "      <td>0.486592</td>\n",
              "      <td>0.836071</td>\n",
              "      <td>14.526497</td>\n",
              "      <td>1.102743</td>\n",
              "      <td>0.806057</td>\n",
              "      <td>49.693429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>223.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>20.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.910400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>446.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.454200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>668.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>891.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>512.329200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\n",
              "count   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\n",
              "mean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\n",
              "std     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\n",
              "min       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n",
              "25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n",
              "50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n",
              "75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\n",
              "max     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgdE4ofo8dzM",
        "colab_type": "text"
      },
      "source": [
        "It is evident that we are missing 177 'Age' values. We need to take care of that.\n",
        "\n",
        "Of course, not all features are useful. In fact, we don't need the following: PassengerID, ticket, Name because they don't tell us anything about the survival rate. It is important to note that the Cabin feature could be useful, however, we will drop it because of its missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPTdouC-8QXz",
        "colab_type": "code",
        "outputId": "e7a5c6c6-0c0a-4c3c-91f2-fe7d84229311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#show train columns\n",
        "train.columns.values"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
              "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZCjTiZqXmMy",
        "colab_type": "code",
        "outputId": "8a407256-aa17-4fed-f757-05a31541129a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#show test columns\n",
        "test.columns.values"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
              "       'Ticket', 'Fare', 'Cabin', 'Embarked'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg6NWCDvtjIO",
        "colab_type": "code",
        "outputId": "b7e88d7d-f7a6-41ef-8410-3c1a8e1dd1d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#obtain the mean age to replace the missing values\n",
        "train.mean(axis=0)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId    446.000000\n",
              "Survived         0.383838\n",
              "Pclass           2.308642\n",
              "Age             29.699118\n",
              "SibSp            0.523008\n",
              "Parch            0.381594\n",
              "Fare            32.204208\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q38Y77ttWiow",
        "colab_type": "code",
        "outputId": "266105c5-e39d-4105-c258-288f359679e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#features\n",
        "train_x = train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'],axis=1)\n",
        "#label\n",
        "train_y = train['Survived']\n",
        "#test dataset, doesn't include the label\n",
        "test_x = test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'],axis=1)\n",
        "#train_y only includes 0 and 1 (dead or alive)\n",
        "print(train_y)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0      0\n",
            "1      1\n",
            "2      1\n",
            "3      1\n",
            "4      0\n",
            "      ..\n",
            "886    0\n",
            "887    1\n",
            "888    0\n",
            "889    1\n",
            "890    0\n",
            "Name: Survived, Length: 891, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecRToJ0t8ECl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fill the Age feature with the mean, which is 30\n",
        "train_x['Age'] = train_x['Age'].fillna(30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPqUgcSf0hB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fill Embarked with S, the most common value\n",
        "train_x[\"Embarked\"].fillna(\"S\", inplace = True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo_8b6D66VtU",
        "colab_type": "code",
        "outputId": "0fa07676-59e1-46cb-fc9e-9804cc38723b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#train_x is the \"complete\" train data\n",
        "train_x.info()"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 8 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Survived  891 non-null    int64  \n",
            " 1   Pclass    891 non-null    int64  \n",
            " 2   Sex       891 non-null    object \n",
            " 3   Age       891 non-null    float64\n",
            " 4   SibSp     891 non-null    int64  \n",
            " 5   Parch     891 non-null    int64  \n",
            " 6   Fare      891 non-null    float64\n",
            " 7   Embarked  891 non-null    object \n",
            "dtypes: float64(2), int64(4), object(2)\n",
            "memory usage: 55.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFYSEXNqhfOE",
        "colab_type": "code",
        "outputId": "0eda2a87-ba02-4a3b-ba4b-2f1e7e4bbee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#indeed, the test_x dataset doesn't include Survived\n",
        "test_x.info()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 418 entries, 0 to 417\n",
            "Data columns (total 7 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Pclass    418 non-null    int64  \n",
            " 1   Sex       418 non-null    object \n",
            " 2   Age       332 non-null    float64\n",
            " 3   SibSp     418 non-null    int64  \n",
            " 4   Parch     418 non-null    int64  \n",
            " 5   Fare      417 non-null    float64\n",
            " 6   Embarked  418 non-null    object \n",
            "dtypes: float64(2), int64(3), object(2)\n",
            "memory usage: 23.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ-j3XmmCa_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b9a40359-e326-4d4f-978b-90a707d11405"
      },
      "source": [
        "test_x.mean(axis=0)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pclass     2.265550\n",
              "Age       30.272590\n",
              "SibSp      0.447368\n",
              "Parch      0.392344\n",
              "Fare      35.627188\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kLKvrXYCfDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_x['Age'] = test_x['Age'].fillna(30)\n",
        "test_x['Fare'] = test_x['Fare'].fillna(36)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7aJwNWcCpNb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "50d6b445-9bf3-448d-f586-110665696d48"
      },
      "source": [
        "test_x.info()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 418 entries, 0 to 417\n",
            "Data columns (total 7 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Pclass    418 non-null    int64  \n",
            " 1   Sex       418 non-null    object \n",
            " 2   Age       418 non-null    float64\n",
            " 3   SibSp     418 non-null    int64  \n",
            " 4   Parch     418 non-null    int64  \n",
            " 5   Fare      418 non-null    float64\n",
            " 6   Embarked  418 non-null    object \n",
            "dtypes: float64(2), int64(3), object(2)\n",
            "memory usage: 23.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3l2fHcaGPEf",
        "colab_type": "text"
      },
      "source": [
        "features = dataframe that includes all the necessary data to determinate if a passenger will survive or not.\n",
        "\n",
        "labels = dataframe containing the information we want to predict. In this example, \"Survived\" is the label.\n",
        "\n",
        "We need to split the train dataset into three parts:\n",
        "\n",
        "1. Training\n",
        "\n",
        "2. Validation\n",
        "\n",
        "3. Test\n",
        "\n",
        "In this example, the Test dataset is already provided. We just need to split \n",
        "the train dataset into: Train and Validation (sometimes called crossvalidation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RBk9dKTelJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sampling 75% for train data and 25% for validation\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, \n",
        "                                                  test_size=0.2, \n",
        "                                                  random_state=1)\n",
        "\n",
        "#create input function\n",
        "train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(x=train_x,\n",
        "                                                               y=train_y,\n",
        "                                                               num_epochs=None,\n",
        "                                                               batch_size=100,\n",
        "                                                               shuffle=True)\n",
        "\n",
        "#sometimes called crossvalidation\n",
        "val_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(x=val_x,\n",
        "                                                             y=val_y,\n",
        "                                                              num_epochs=1,\n",
        "                                                              batch_size=len(val_x),\n",
        "                                                              shuffle=False)\n",
        "\n",
        "                                                      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty1KB2yThQR2",
        "colab_type": "text"
      },
      "source": [
        "**Feature Columns**\n",
        "\n",
        "In this example, we have Categorical columns and numerical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNfmhXM1HQzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the layman implementation of feature columns\n",
        "Sex = tf.feature_column.categorical_column_with_vocabulary_list(\"Sex\", [\"male\", \"female\"])\n",
        "Embarked = tf.feature_column.categorical_column_with_vocabulary_list(\"Embarked\", [\"S\", \"C\", \"Q\"])\n",
        "Age = tf.feature_column.numeric_column(\"Age\")\n",
        "Fare = tf.feature_column.numeric_column(\"Fare\")\n",
        "Parch = tf.feature_column.numeric_column(\"Parch\")\n",
        "Pclass = tf.feature_column.numeric_column(\"Pclass\")\n",
        "SibSp = tf.feature_column.numeric_column(\"SibSp\")\n",
        "\n",
        "ft_columns = [tf.feature_column.indicator_column(Sex), tf.feature_column.indicator_column(Embarked), Age, Fare, Parch, Pclass, SibSp]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7WLsP3_Pyha",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "feature_columns = []\n",
        "num_cols = ['Age', 'Fare', 'Parch', 'Pclass', 'SibSp']\n",
        "for num_name in num_cols:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(num_name))\n",
        "\n",
        "categorical_cols = ['Sex', 'Embarked']\n",
        "for ft_name in categorical_cols:\n",
        "  vocabulary = train_x[ft_name].unique()\n",
        "  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(ft_name, vocabulary))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkNp1UfcmIOY",
        "colab_type": "text"
      },
      "source": [
        "**Instantiate the Estimator**\n",
        "\n",
        "We are going to use a Linear Classifier. This will train the model to classify the data into two possible cases: survival and not survival.\n",
        "\n",
        "hidden_units = number of hidden nodes per layer. Ex. [30,10] means that we have two layers, the first one with 30 nodes and the second one with 10 nodes.\n",
        "\n",
        "n_classes= number of label classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4vYPRV5ltaA",
        "colab_type": "code",
        "outputId": "d6ccfb22-3ce0-4028-a954-478d2ceb5113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "model = tf.estimator.DNNClassifier(feature_columns=ft_columns,\n",
        "                                   hidden_units=[30,10,30], \n",
        "                                   n_classes=2, \n",
        "                                   optimizer='Adam')"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpqyk1s3fw\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpqyk1s3fw', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4iovzZ-nsZk",
        "colab_type": "text"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03dsVfrbnrh1",
        "colab_type": "code",
        "outputId": "74aa45e1-6d55-4046-d585-5378eaf307d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "model.train(input_fn=train_input_fn, steps=500)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpqyk1s3fw/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
            "INFO:tensorflow:loss = 0.9487112, step = 0\n",
            "INFO:tensorflow:global_step/sec: 263.036\n",
            "INFO:tensorflow:loss = 0.6111293, step = 100 (0.382 sec)\n",
            "INFO:tensorflow:global_step/sec: 319.992\n",
            "INFO:tensorflow:loss = 0.4743426, step = 200 (0.314 sec)\n",
            "INFO:tensorflow:global_step/sec: 333.37\n",
            "INFO:tensorflow:loss = 0.43130112, step = 301 (0.300 sec)\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 301 vs previous value: 301. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:global_step/sec: 332.549\n",
            "INFO:tensorflow:loss = 0.45481735, step = 400 (0.304 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 500...\n",
            "INFO:tensorflow:Saving checkpoints for 500 into /tmp/tmpqyk1s3fw/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 500...\n",
            "INFO:tensorflow:Loss for final step: 0.4083898.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x7fcd01e3e1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W6pe2wQRwVw",
        "colab_type": "text"
      },
      "source": [
        "**Evaluate the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-Z2pFahR0kS",
        "colab_type": "code",
        "outputId": "0e45066a-7d60-4899-d3fc-ecc0723f9c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "accuracy_score = model.evaluate(input_fn=val_input_fn)[\"accuracy\"]\n",
        "print(\"\\nTest accuracy: {0:f}%\\n\".format(accuracy_score*100))"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-04-28T19:31:05Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpqyk1s3fw/model.ckpt-500\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Inference Time : 0.55127s\n",
            "INFO:tensorflow:Finished evaluation at 2020-04-28-19:31:05\n",
            "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.7765363, accuracy_baseline = 0.59217876, auc = 0.82818556, auc_precision_recall = 0.7981224, average_loss = 0.49084753, global_step = 500, label/mean = 0.40782124, loss = 0.49084753, precision = 0.7704918, prediction/mean = 0.35866538, recall = 0.6438356\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: /tmp/tmpqyk1s3fw/model.ckpt-500\n",
            "\n",
            "Test accuracy: 77.653629%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muul7m33qq3r",
        "colab_type": "text"
      },
      "source": [
        "**TEST DATA (VALIDATION)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWCzZ0gMQaI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(x=test_x,\n",
        "                                                              num_epochs=1,\n",
        "                                                              batch_size=len(test_x),\n",
        "                                                              shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "patHxi9Fqt1S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8fd1334-357a-4e0d-ecfa-e0a16182cfae"
      },
      "source": [
        "LABEL =['YOU DIED', 'YOU LIVED']\n",
        "predictions = model.predict(input_fn=test_input_fn)\n",
        "for i, predict in enumerate(predictions):\n",
        "    label_ = predict['class_ids'][0]\n",
        "    probs = predict['probabilities'][label_]\n",
        "    print(f'The prediction is \\t{LABEL[label_]} ({100*probs} %) ')\n"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpqyk1s3fw/model.ckpt-500\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "The prediction is \tYOU DIED (89.25419449806213 %) \n",
            "The prediction is \tYOU DIED (50.411832332611084 %) \n",
            "The prediction is \tYOU DIED (83.88999700546265 %) \n",
            "The prediction is \tYOU DIED (91.08308553695679 %) \n",
            "The prediction is \tYOU DIED (54.66517210006714 %) \n",
            "The prediction is \tYOU DIED (81.95133209228516 %) \n",
            "The prediction is \tYOU LIVED (68.38501691818237 %) \n",
            "The prediction is \tYOU DIED (89.57264423370361 %) \n",
            "The prediction is \tYOU LIVED (65.82925915718079 %) \n",
            "The prediction is \tYOU DIED (92.55040884017944 %) \n",
            "The prediction is \tYOU DIED (91.33231043815613 %) \n",
            "The prediction is \tYOU DIED (75.78755617141724 %) \n",
            "The prediction is \tYOU LIVED (94.19897198677063 %) \n",
            "The prediction is \tYOU DIED (94.45154070854187 %) \n",
            "The prediction is \tYOU LIVED (96.01868391036987 %) \n",
            "The prediction is \tYOU LIVED (88.34460377693176 %) \n",
            "The prediction is \tYOU DIED (85.88839769363403 %) \n",
            "The prediction is \tYOU DIED (84.84556674957275 %) \n",
            "The prediction is \tYOU DIED (59.22700762748718 %) \n",
            "The prediction is \tYOU LIVED (69.55591440200806 %) \n",
            "The prediction is \tYOU DIED (58.33917260169983 %) \n",
            "The prediction is \tYOU DIED (67.03876852989197 %) \n",
            "The prediction is \tYOU LIVED (91.57261252403259 %) \n",
            "The prediction is \tYOU LIVED (53.745102882385254 %) \n",
            "The prediction is \tYOU LIVED (87.09176182746887 %) \n",
            "The prediction is \tYOU DIED (96.61628007888794 %) \n",
            "The prediction is \tYOU LIVED (97.64710664749146 %) \n",
            "The prediction is \tYOU DIED (85.45706272125244 %) \n",
            "The prediction is \tYOU DIED (73.87006878852844 %) \n",
            "The prediction is \tYOU DIED (94.45796608924866 %) \n",
            "The prediction is \tYOU DIED (91.77876114845276 %) \n",
            "The prediction is \tYOU DIED (87.6821219921112 %) \n",
            "The prediction is \tYOU DIED (55.846768617630005 %) \n",
            "The prediction is \tYOU DIED (53.31681966781616 %) \n",
            "The prediction is \tYOU DIED (59.210169315338135 %) \n",
            "The prediction is \tYOU DIED (83.30112099647522 %) \n",
            "The prediction is \tYOU LIVED (50.34978985786438 %) \n",
            "The prediction is \tYOU LIVED (58.703213930130005 %) \n",
            "The prediction is \tYOU DIED (91.09039306640625 %) \n",
            "The prediction is \tYOU DIED (77.64465808868408 %) \n",
            "The prediction is \tYOU DIED (93.41288805007935 %) \n",
            "The prediction is \tYOU DIED (75.13757944107056 %) \n",
            "The prediction is \tYOU DIED (92.38477945327759 %) \n",
            "The prediction is \tYOU LIVED (71.86687588691711 %) \n",
            "The prediction is \tYOU LIVED (92.476886510849 %) \n",
            "The prediction is \tYOU DIED (90.2065098285675 %) \n",
            "The prediction is \tYOU DIED (63.38350772857666 %) \n",
            "The prediction is \tYOU DIED (88.60406875610352 %) \n",
            "The prediction is \tYOU LIVED (98.70951771736145 %) \n",
            "The prediction is \tYOU DIED (52.44729518890381 %) \n",
            "The prediction is \tYOU DIED (73.05220365524292 %) \n",
            "The prediction is \tYOU DIED (83.17331671714783 %) \n",
            "The prediction is \tYOU LIVED (67.69912242889404 %) \n",
            "The prediction is \tYOU LIVED (63.65419626235962 %) \n",
            "The prediction is \tYOU DIED (83.95195603370667 %) \n",
            "The prediction is \tYOU DIED (89.66740369796753 %) \n",
            "The prediction is \tYOU DIED (92.35280156135559 %) \n",
            "The prediction is \tYOU DIED (90.04437923431396 %) \n",
            "The prediction is \tYOU DIED (95.23119926452637 %) \n",
            "The prediction is \tYOU LIVED (95.35292983055115 %) \n",
            "The prediction is \tYOU DIED (85.2959394454956 %) \n",
            "The prediction is \tYOU DIED (89.4875168800354 %) \n",
            "The prediction is \tYOU DIED (86.11968159675598 %) \n",
            "The prediction is \tYOU LIVED (68.98501515388489 %) \n",
            "The prediction is \tYOU LIVED (74.69152212142944 %) \n",
            "The prediction is \tYOU LIVED (81.1706006526947 %) \n",
            "The prediction is \tYOU LIVED (71.9245970249176 %) \n",
            "The prediction is \tYOU DIED (68.92028450965881 %) \n",
            "The prediction is \tYOU DIED (63.28156590461731 %) \n",
            "The prediction is \tYOU LIVED (88.56483697891235 %) \n",
            "The prediction is \tYOU LIVED (67.62019991874695 %) \n",
            "The prediction is \tYOU DIED (88.73107433319092 %) \n",
            "The prediction is \tYOU LIVED (50.92823505401611 %) \n",
            "The prediction is \tYOU DIED (63.493967056274414 %) \n",
            "The prediction is \tYOU LIVED (96.2000846862793 %) \n",
            "The prediction is \tYOU LIVED (50.95023512840271 %) \n",
            "The prediction is \tYOU DIED (91.41284227371216 %) \n",
            "The prediction is \tYOU LIVED (58.77903699874878 %) \n",
            "The prediction is \tYOU DIED (89.16221857070923 %) \n",
            "The prediction is \tYOU LIVED (67.62019991874695 %) \n",
            "The prediction is \tYOU DIED (65.79027771949768 %) \n",
            "The prediction is \tYOU LIVED (77.12628245353699 %) \n",
            "The prediction is \tYOU DIED (77.52912640571594 %) \n",
            "The prediction is \tYOU DIED (91.33231043815613 %) \n",
            "The prediction is \tYOU DIED (85.18083691596985 %) \n",
            "The prediction is \tYOU DIED (93.12098026275635 %) \n",
            "The prediction is \tYOU LIVED (67.64529347419739 %) \n",
            "The prediction is \tYOU LIVED (59.97423529624939 %) \n",
            "The prediction is \tYOU LIVED (68.24289560317993 %) \n",
            "The prediction is \tYOU LIVED (51.976341009140015 %) \n",
            "The prediction is \tYOU LIVED (54.87397909164429 %) \n",
            "The prediction is \tYOU DIED (91.26874208450317 %) \n",
            "The prediction is \tYOU LIVED (84.7507894039154 %) \n",
            "The prediction is \tYOU DIED (91.41284227371216 %) \n",
            "The prediction is \tYOU DIED (63.395196199417114 %) \n",
            "The prediction is \tYOU DIED (90.13062715530396 %) \n",
            "The prediction is \tYOU LIVED (94.342440366745 %) \n",
            "The prediction is \tYOU DIED (91.129469871521 %) \n",
            "The prediction is \tYOU LIVED (58.02866816520691 %) \n",
            "The prediction is \tYOU DIED (92.03410744667053 %) \n",
            "The prediction is \tYOU LIVED (96.74296975135803 %) \n",
            "The prediction is \tYOU DIED (88.0212664604187 %) \n",
            "The prediction is \tYOU DIED (88.60406875610352 %) \n",
            "The prediction is \tYOU DIED (90.35874605178833 %) \n",
            "The prediction is \tYOU LIVED (52.909791469573975 %) \n",
            "The prediction is \tYOU DIED (90.40254950523376 %) \n",
            "The prediction is \tYOU DIED (85.93610525131226 %) \n",
            "The prediction is \tYOU DIED (88.60406875610352 %) \n",
            "The prediction is \tYOU DIED (91.75116419792175 %) \n",
            "The prediction is \tYOU DIED (86.27192974090576 %) \n",
            "The prediction is \tYOU DIED (88.52851390838623 %) \n",
            "The prediction is \tYOU LIVED (68.20850372314453 %) \n",
            "The prediction is \tYOU LIVED (93.82810592651367 %) \n",
            "The prediction is \tYOU LIVED (70.684415102005 %) \n",
            "The prediction is \tYOU LIVED (98.86994957923889 %) \n",
            "The prediction is \tYOU DIED (89.56315517425537 %) \n",
            "The prediction is \tYOU DIED (86.9967520236969 %) \n",
            "The prediction is \tYOU LIVED (71.1676299571991 %) \n",
            "The prediction is \tYOU LIVED (55.444592237472534 %) \n",
            "The prediction is \tYOU LIVED (76.80820226669312 %) \n",
            "The prediction is \tYOU LIVED (88.18770051002502 %) \n",
            "The prediction is \tYOU DIED (90.78114628791809 %) \n",
            "The prediction is \tYOU LIVED (97.25072979927063 %) \n",
            "The prediction is \tYOU DIED (90.52987694740295 %) \n",
            "The prediction is \tYOU DIED (88.60406875610352 %) \n",
            "The prediction is \tYOU LIVED (70.26587724685669 %) \n",
            "The prediction is \tYOU DIED (89.11275863647461 %) \n",
            "The prediction is \tYOU LIVED (67.07389950752258 %) \n",
            "The prediction is \tYOU DIED (92.34093427658081 %) \n",
            "The prediction is \tYOU DIED (90.00129699707031 %) \n",
            "The prediction is \tYOU DIED (91.83172583580017 %) \n",
            "The prediction is \tYOU DIED (70.7535982131958 %) \n",
            "The prediction is \tYOU DIED (54.77350950241089 %) \n",
            "The prediction is \tYOU DIED (89.38081860542297 %) \n",
            "The prediction is \tYOU DIED (92.33355522155762 %) \n",
            "The prediction is \tYOU DIED (89.87569212913513 %) \n",
            "The prediction is \tYOU DIED (86.76193356513977 %) \n",
            "The prediction is \tYOU DIED (87.86841630935669 %) \n",
            "The prediction is \tYOU LIVED (56.00025653839111 %) \n",
            "The prediction is \tYOU DIED (97.40310907363892 %) \n",
            "The prediction is \tYOU DIED (75.35172700881958 %) \n",
            "The prediction is \tYOU LIVED (96.58921360969543 %) \n",
            "The prediction is \tYOU DIED (56.28693699836731 %) \n",
            "The prediction is \tYOU DIED (83.44150185585022 %) \n",
            "The prediction is \tYOU DIED (74.74368810653687 %) \n",
            "The prediction is \tYOU DIED (96.91874384880066 %) \n",
            "The prediction is \tYOU DIED (54.720938205718994 %) \n",
            "The prediction is \tYOU DIED (89.28636908531189 %) \n",
            "The prediction is \tYOU DIED (75.13757944107056 %) \n",
            "The prediction is \tYOU DIED (92.51645803451538 %) \n",
            "The prediction is \tYOU LIVED (97.24940061569214 %) \n",
            "The prediction is \tYOU DIED (88.28840255737305 %) \n",
            "The prediction is \tYOU DIED (87.8355622291565 %) \n",
            "The prediction is \tYOU DIED (59.2629611492157 %) \n",
            "The prediction is \tYOU DIED (92.48800873756409 %) \n",
            "The prediction is \tYOU DIED (89.67777490615845 %) \n",
            "The prediction is \tYOU LIVED (92.13130474090576 %) \n",
            "The prediction is \tYOU LIVED (55.640554428100586 %) \n",
            "The prediction is \tYOU DIED (74.74368810653687 %) \n",
            "The prediction is \tYOU LIVED (52.55080461502075 %) \n",
            "The prediction is \tYOU LIVED (68.26257109642029 %) \n",
            "The prediction is \tYOU DIED (70.17971277236938 %) \n",
            "The prediction is \tYOU LIVED (75.11831521987915 %) \n",
            "The prediction is \tYOU DIED (90.85085391998291 %) \n",
            "The prediction is \tYOU DIED (92.11261868476868 %) \n",
            "The prediction is \tYOU LIVED (57.256877422332764 %) \n",
            "The prediction is \tYOU DIED (51.28304958343506 %) \n",
            "The prediction is \tYOU DIED (91.79962873458862 %) \n",
            "The prediction is \tYOU LIVED (93.37226152420044 %) \n",
            "The prediction is \tYOU LIVED (58.45802426338196 %) \n",
            "The prediction is \tYOU DIED (91.14923477172852 %) \n",
            "The prediction is \tYOU DIED (86.91788911819458 %) \n",
            "The prediction is \tYOU DIED (93.49382519721985 %) \n",
            "The prediction is \tYOU DIED (87.82400488853455 %) \n",
            "The prediction is \tYOU DIED (98.43756556510925 %) \n",
            "The prediction is \tYOU LIVED (88.3926272392273 %) \n",
            "The prediction is \tYOU LIVED (92.84160137176514 %) \n",
            "The prediction is \tYOU DIED (61.62658333778381 %) \n",
            "The prediction is \tYOU LIVED (76.24848484992981 %) \n",
            "The prediction is \tYOU LIVED (98.35503101348877 %) \n",
            "The prediction is \tYOU DIED (89.16221857070923 %) \n",
            "The prediction is \tYOU DIED (66.1754846572876 %) \n",
            "The prediction is \tYOU LIVED (93.86500120162964 %) \n",
            "The prediction is \tYOU DIED (88.60406875610352 %) \n",
            "The prediction is \tYOU LIVED (90.02197980880737 %) \n",
            "The prediction is \tYOU DIED (90.43411612510681 %) \n",
            "The prediction is \tYOU LIVED (79.74932789802551 %) \n",
            "The prediction is \tYOU DIED (88.45536708831787 %) \n",
            "The prediction is \tYOU DIED (98.64023327827454 %) \n",
            "The prediction is \tYOU DIED (91.87811613082886 %) \n",
            "The prediction is \tYOU DIED (89.61701393127441 %) \n",
            "The prediction is \tYOU DIED (75.35457611083984 %) \n",
            "The prediction is \tYOU DIED (83.40995907783508 %) \n",
            "The prediction is \tYOU DIED (88.31059336662292 %) \n",
            "The prediction is \tYOU LIVED (50.62796473503113 %) \n",
            "The prediction is \tYOU DIED (91.93849563598633 %) \n",
            "The prediction is \tYOU LIVED (75.87624192237854 %) \n",
            "The prediction is \tYOU LIVED (59.45865511894226 %) \n",
            "The prediction is \tYOU DIED (88.41560482978821 %) \n",
            "The prediction is \tYOU LIVED (50.46616196632385 %) \n",
            "The prediction is \tYOU LIVED (75.38004517555237 %) \n",
            "The prediction is \tYOU LIVED (61.620932817459106 %) \n",
            "The prediction is \tYOU LIVED (51.962810754776 %) \n",
            "The prediction is \tYOU LIVED (80.3841769695282 %) \n",
            "The prediction is \tYOU DIED (89.05222415924072 %) \n",
            "The prediction is \tYOU DIED (64.61127996444702 %) \n",
            "The prediction is \tYOU LIVED (68.97574067115784 %) \n",
            "The prediction is \tYOU DIED (88.73783946037292 %) \n",
            "The prediction is \tYOU LIVED (92.8442120552063 %) \n",
            "The prediction is \tYOU DIED (90.18939733505249 %) \n",
            "The prediction is \tYOU DIED (90.36478996276855 %) \n",
            "The prediction is \tYOU DIED (90.87834358215332 %) \n",
            "The prediction is \tYOU DIED (55.927276611328125 %) \n",
            "The prediction is \tYOU LIVED (58.56524705886841 %) \n",
            "The prediction is \tYOU DIED (80.13690114021301 %) \n",
            "The prediction is \tYOU DIED (68.7096357345581 %) \n",
            "The prediction is \tYOU LIVED (68.09054613113403 %) \n",
            "The prediction is \tYOU LIVED (60.62074303627014 %) \n",
            "The prediction is \tYOU LIVED (96.70502543449402 %) \n",
            "The prediction is \tYOU DIED (91.41284227371216 %) \n",
            "The prediction is \tYOU LIVED (67.12226271629333 %) \n",
            "The prediction is \tYOU DIED (88.85156512260437 %) \n",
            "The prediction is \tYOU LIVED (73.32674860954285 %) \n",
            "The prediction is \tYOU DIED (88.6523187160492 %) \n",
            "The prediction is \tYOU LIVED (88.12670111656189 %) \n",
            "The prediction is \tYOU LIVED (58.67341160774231 %) \n",
            "The prediction is \tYOU DIED (89.48087096214294 %) \n",
            "The prediction is \tYOU LIVED (68.24289560317993 %) \n",
            "The prediction is \tYOU DIED (94.42678093910217 %) \n",
            "The prediction is \tYOU DIED (90.8753514289856 %) \n",
            "The prediction is \tYOU DIED (56.11810088157654 %) \n",
            "The prediction is \tYOU LIVED (93.332839012146 %) \n",
            "The prediction is \tYOU DIED (89.3526554107666 %) \n",
            "The prediction is \tYOU DIED (88.66478204727173 %) \n",
            "The prediction is \tYOU LIVED (53.56542468070984 %) \n",
            "The prediction is \tYOU DIED (87.94993758201599 %) \n",
            "The prediction is \tYOU LIVED (54.5484721660614 %) \n",
            "The prediction is \tYOU DIED (84.34361815452576 %) \n",
            "The prediction is \tYOU LIVED (63.36846351623535 %) \n",
            "The prediction is \tYOU LIVED (97.53690361976624 %) \n",
            "The prediction is \tYOU LIVED (87.60815858840942 %) \n",
            "The prediction is \tYOU LIVED (72.4824070930481 %) \n",
            "The prediction is \tYOU LIVED (59.638911485672 %) \n",
            "The prediction is \tYOU DIED (91.32795929908752 %) \n",
            "The prediction is \tYOU DIED (96.30320072174072 %) \n",
            "The prediction is \tYOU DIED (62.603431940078735 %) \n",
            "The prediction is \tYOU LIVED (83.7340772151947 %) \n",
            "The prediction is \tYOU DIED (91.88345074653625 %) \n",
            "The prediction is \tYOU LIVED (76.80820226669312 %) \n",
            "The prediction is \tYOU LIVED (55.05736470222473 %) \n",
            "The prediction is \tYOU LIVED (74.13955926895142 %) \n",
            "The prediction is \tYOU DIED (88.00275325775146 %) \n",
            "The prediction is \tYOU DIED (60.43168306350708 %) \n",
            "The prediction is \tYOU DIED (90.7697319984436 %) \n",
            "The prediction is \tYOU DIED (92.61776804924011 %) \n",
            "The prediction is \tYOU DIED (91.14923477172852 %) \n",
            "The prediction is \tYOU DIED (88.60406875610352 %) \n",
            "The prediction is \tYOU DIED (90.97459316253662 %) \n",
            "The prediction is \tYOU LIVED (79.22043800354004 %) \n",
            "The prediction is \tYOU DIED (88.63587975502014 %) \n",
            "The prediction is \tYOU DIED (95.84052562713623 %) \n",
            "The prediction is \tYOU DIED (88.69837522506714 %) \n",
            "The prediction is \tYOU LIVED (70.14525532722473 %) \n",
            "The prediction is \tYOU LIVED (70.64477205276489 %) \n",
            "The prediction is \tYOU DIED (86.17255687713623 %) \n",
            "The prediction is \tYOU DIED (91.33231043815613 %) \n",
            "The prediction is \tYOU DIED (92.28444695472717 %) \n",
            "The prediction is \tYOU DIED (91.14923477172852 %) \n",
            "The prediction is \tYOU LIVED (50.34978985786438 %) \n",
            "The prediction is \tYOU DIED (85.96109747886658 %) \n",
            "The prediction is \tYOU LIVED (69.86579895019531 %) \n",
            "The prediction is \tYOU DIED (88.60406875610352 %) \n",
            "The prediction is \tYOU LIVED (93.35084557533264 %) \n",
            "The prediction is \tYOU LIVED (63.00276517868042 %) \n",
            "The prediction is \tYOU DIED (87.82102465629578 %) \n",
            "The prediction is \tYOU LIVED (85.8193039894104 %) \n",
            "The prediction is \tYOU DIED (89.94954228401184 %) \n",
            "The prediction is \tYOU DIED (89.68689441680908 %) \n",
            "The prediction is \tYOU DIED (90.50920009613037 %) \n",
            "The prediction is \tYOU DIED (88.08539509773254 %) \n",
            "The prediction is \tYOU LIVED (57.12127685546875 %) \n",
            "The prediction is \tYOU LIVED (55.49898147583008 %) \n",
            "The prediction is \tYOU LIVED (68.24289560317993 %) \n",
            "The prediction is \tYOU LIVED (79.49865460395813 %) \n",
            "The prediction is \tYOU LIVED (70.64052820205688 %) \n",
            "The prediction is \tYOU DIED (91.83037281036377 %) \n",
            "The prediction is \tYOU DIED (90.98758101463318 %) \n",
            "The prediction is \tYOU DIED (62.82984018325806 %) \n",
            "The prediction is \tYOU DIED (87.82400488853455 %) \n",
            "The prediction is \tYOU DIED (91.41284227371216 %) \n",
            "The prediction is \tYOU DIED (57.954782247543335 %) \n",
            "The prediction is \tYOU LIVED (69.17761564254761 %) \n",
            "The prediction is \tYOU DIED (87.82400488853455 %) \n",
            "The prediction is \tYOU DIED (56.50118589401245 %) \n",
            "The prediction is \tYOU DIED (93.1200385093689 %) \n",
            "The prediction is \tYOU DIED (90.42823910713196 %) \n",
            "The prediction is \tYOU LIVED (76.72663927078247 %) \n",
            "The prediction is \tYOU DIED (94.45796608924866 %) \n",
            "The prediction is \tYOU DIED (56.36301636695862 %) \n",
            "The prediction is \tYOU DIED (91.09147787094116 %) \n",
            "The prediction is \tYOU DIED (91.69402718544006 %) \n",
            "The prediction is \tYOU DIED (84.4635546207428 %) \n",
            "The prediction is \tYOU DIED (90.2235746383667 %) \n",
            "The prediction is \tYOU DIED (90.38530588150024 %) \n",
            "The prediction is \tYOU LIVED (68.24289560317993 %) \n",
            "The prediction is \tYOU LIVED (62.67361640930176 %) \n",
            "The prediction is \tYOU DIED (72.95206785202026 %) \n",
            "The prediction is \tYOU LIVED (57.756924629211426 %) \n",
            "The prediction is \tYOU DIED (52.94148325920105 %) \n",
            "The prediction is \tYOU DIED (64.75210785865784 %) \n",
            "The prediction is \tYOU DIED (86.8710458278656 %) \n",
            "The prediction is \tYOU DIED (85.28665900230408 %) \n",
            "The prediction is \tYOU DIED (91.1625862121582 %) \n",
            "The prediction is \tYOU LIVED (69.27899122238159 %) \n",
            "The prediction is \tYOU LIVED (99.47742819786072 %) \n",
            "The prediction is \tYOU LIVED (72.35838770866394 %) \n",
            "The prediction is \tYOU LIVED (72.02158570289612 %) \n",
            "The prediction is \tYOU DIED (86.98197603225708 %) \n",
            "The prediction is \tYOU DIED (90.6385064125061 %) \n",
            "The prediction is \tYOU DIED (87.30207085609436 %) \n",
            "The prediction is \tYOU DIED (90.35874605178833 %) \n",
            "The prediction is \tYOU DIED (86.28794550895691 %) \n",
            "The prediction is \tYOU DIED (87.86841630935669 %) \n",
            "The prediction is \tYOU DIED (75.22372007369995 %) \n",
            "The prediction is \tYOU LIVED (95.67447900772095 %) \n",
            "The prediction is \tYOU DIED (88.97274136543274 %) \n",
            "The prediction is \tYOU LIVED (70.28592824935913 %) \n",
            "The prediction is \tYOU LIVED (70.72966694831848 %) \n",
            "The prediction is \tYOU DIED (89.04854655265808 %) \n",
            "The prediction is \tYOU DIED (86.3458514213562 %) \n",
            "The prediction is \tYOU LIVED (73.4797716140747 %) \n",
            "The prediction is \tYOU DIED (62.995028495788574 %) \n",
            "The prediction is \tYOU DIED (87.82102465629578 %) \n",
            "The prediction is \tYOU LIVED (66.90247654914856 %) \n",
            "The prediction is \tYOU DIED (90.6618595123291 %) \n",
            "The prediction is \tYOU DIED (75.35457611083984 %) \n",
            "The prediction is \tYOU DIED (89.76256847381592 %) \n",
            "The prediction is \tYOU DIED (91.2783682346344 %) \n",
            "The prediction is \tYOU DIED (74.9076247215271 %) \n",
            "The prediction is \tYOU DIED (87.82400488853455 %) \n",
            "The prediction is \tYOU DIED (86.44133806228638 %) \n",
            "The prediction is \tYOU DIED (91.59470200538635 %) \n",
            "The prediction is \tYOU DIED (85.60458421707153 %) \n",
            "The prediction is \tYOU LIVED (96.8190848827362 %) \n",
            "The prediction is \tYOU DIED (95.48971056938171 %) \n",
            "The prediction is \tYOU LIVED (61.756956577301025 %) \n",
            "The prediction is \tYOU DIED (87.86841630935669 %) \n",
            "The prediction is \tYOU LIVED (64.3484890460968 %) \n",
            "The prediction is \tYOU DIED (86.83846592903137 %) \n",
            "The prediction is \tYOU LIVED (80.99303841590881 %) \n",
            "The prediction is \tYOU LIVED (98.04941415786743 %) \n",
            "The prediction is \tYOU DIED (89.05222415924072 %) \n",
            "The prediction is \tYOU DIED (55.0346314907074 %) \n",
            "The prediction is \tYOU DIED (78.58961224555969 %) \n",
            "The prediction is \tYOU LIVED (72.12997674942017 %) \n",
            "The prediction is \tYOU DIED (78.08946967124939 %) \n",
            "The prediction is \tYOU LIVED (83.5361123085022 %) \n",
            "The prediction is \tYOU DIED (91.32360219955444 %) \n",
            "The prediction is \tYOU DIED (88.60406875610352 %) \n",
            "The prediction is \tYOU DIED (50.34855008125305 %) \n",
            "The prediction is \tYOU DIED (97.144216299057 %) \n",
            "The prediction is \tYOU LIVED (90.9913182258606 %) \n",
            "The prediction is \tYOU LIVED (80.99303841590881 %) \n",
            "The prediction is \tYOU DIED (91.08308553695679 %) \n",
            "The prediction is \tYOU LIVED (95.61347365379333 %) \n",
            "The prediction is \tYOU DIED (54.36614155769348 %) \n",
            "The prediction is \tYOU DIED (93.12209486961365 %) \n",
            "The prediction is \tYOU LIVED (89.77947235107422 %) \n",
            "The prediction is \tYOU LIVED (97.78780937194824 %) \n",
            "The prediction is \tYOU DIED (84.96093153953552 %) \n",
            "The prediction is \tYOU DIED (91.19943380355835 %) \n",
            "The prediction is \tYOU LIVED (97.88793325424194 %) \n",
            "The prediction is \tYOU DIED (95.69640159606934 %) \n",
            "The prediction is \tYOU DIED (92.77944564819336 %) \n",
            "The prediction is \tYOU LIVED (96.63019180297852 %) \n",
            "The prediction is \tYOU LIVED (96.88395261764526 %) \n",
            "The prediction is \tYOU DIED (67.05098748207092 %) \n",
            "The prediction is \tYOU DIED (87.09665536880493 %) \n",
            "The prediction is \tYOU DIED (65.64977169036865 %) \n",
            "The prediction is \tYOU DIED (72.3753571510315 %) \n",
            "The prediction is \tYOU DIED (88.60406875610352 %) \n",
            "The prediction is \tYOU DIED (87.5481128692627 %) \n",
            "The prediction is \tYOU LIVED (62.11627721786499 %) \n",
            "The prediction is \tYOU LIVED (65.23817777633667 %) \n",
            "The prediction is \tYOU DIED (89.23348784446716 %) \n",
            "The prediction is \tYOU LIVED (86.96750402450562 %) \n",
            "The prediction is \tYOU DIED (89.82449173927307 %) \n",
            "The prediction is \tYOU DIED (93.75479221343994 %) \n",
            "The prediction is \tYOU DIED (85.87931394577026 %) \n",
            "The prediction is \tYOU DIED (75.09818077087402 %) \n",
            "The prediction is \tYOU LIVED (50.612056255340576 %) \n",
            "The prediction is \tYOU LIVED (87.97916769981384 %) \n",
            "The prediction is \tYOU DIED (77.17540264129639 %) \n",
            "The prediction is \tYOU DIED (92.75023937225342 %) \n",
            "The prediction is \tYOU DIED (97.39006757736206 %) \n",
            "The prediction is \tYOU LIVED (93.44556331634521 %) \n",
            "The prediction is \tYOU DIED (86.46021485328674 %) \n",
            "The prediction is \tYOU LIVED (97.47280478477478 %) \n",
            "The prediction is \tYOU DIED (89.09844756126404 %) \n",
            "The prediction is \tYOU DIED (88.81540894508362 %) \n",
            "The prediction is \tYOU LIVED (94.87668871879578 %) \n",
            "The prediction is \tYOU DIED (91.64974689483643 %) \n",
            "The prediction is \tYOU LIVED (97.55188822746277 %) \n",
            "The prediction is \tYOU DIED (56.10162019729614 %) \n",
            "The prediction is \tYOU DIED (76.91645622253418 %) \n",
            "The prediction is \tYOU DIED (80.91750144958496 %) \n",
            "The prediction is \tYOU DIED (91.48362874984741 %) \n",
            "The prediction is \tYOU LIVED (54.43463921546936 %) \n",
            "The prediction is \tYOU LIVED (68.27729344367981 %) \n",
            "The prediction is \tYOU LIVED (70.0036883354187 %) \n",
            "The prediction is \tYOU LIVED (68.24289560317993 %) \n",
            "The prediction is \tYOU LIVED (97.37078547477722 %) \n",
            "The prediction is \tYOU LIVED (51.459938287734985 %) \n",
            "The prediction is \tYOU DIED (91.41284227371216 %) \n",
            "The prediction is \tYOU LIVED (99.22492504119873 %) \n",
            "The prediction is \tYOU DIED (91.8540894985199 %) \n",
            "The prediction is \tYOU DIED (91.41284227371216 %) \n",
            "The prediction is \tYOU DIED (93.1367576122284 %) \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}